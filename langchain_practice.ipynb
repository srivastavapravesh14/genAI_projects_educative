{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IkCWxGWEK8y",
        "outputId": "955c021a-09f3-474e-83e1-78d892c3c28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token present for HUGGINGFACE\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Ensure the token is set as an environment variable\n",
        "if hf_token:\n",
        "  os.environ['HUGGINGFACEHUB_API_TOKEN'] = hf_token\n",
        "  # login(token = hf_token)\n",
        "def check_env():\n",
        "  required = \"HUGGINGFACEHUB_API_TOKEN\"\n",
        "  if required not in os.environ:\n",
        "    raise EnvironmentError(\n",
        "        f\"{required} not found. Please add it via colab Secrets.\"\n",
        "    )\n",
        "  else:\n",
        "    print(\"Token present for HUGGINGFACE\")\n",
        "\n",
        "check_env()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNys0kRFMUsC",
        "outputId": "6502139c-60a7-4487-fc19-d507401ddaa2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.2.6)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.16.0)\n",
            "Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic model Invoke and test"
      ],
      "metadata": {
        "id": "qLMv1a-v9Ez0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "model = HuggingFaceEndpoint(\n",
        "    repo_id = \"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    task = \"text-generation\",\n",
        "    max_new_tokens = 512,\n",
        "    do_sample = False,\n",
        "    repetition_penalty = 1.03,\n",
        ")\n",
        "\n",
        "chat = ChatHuggingFace(llm = model)"
      ],
      "metadata": {
        "id": "F55EF4YyEm_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \" you are a helpful AI Assistant\"\n",
        "}]\n",
        "\n",
        "message = \"what is the largest island on this world? \"\n",
        "response = chat.invoke(message)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "9rmQ8qDfx-AZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ddbe64-a64a-4818-d7c8-a84e3b09f042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The largest island in the world is Greenland, with an area of approximately 2,166,086 square kilometers (836,330 square miles). It is an autonomous territory within the Kingdom of Denmark and is located between the Arctic and Atlantic Oceans. Despite its name, Greenland is mostly covered in ice, with about 80% of its surface being ice cap.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Different Message styles"
      ],
      "metadata": {
        "id": "OF9wqGvH9K0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage,AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content = \"you are a chef with sense of humor\"),\n",
        "    HumanMessage(content = \"help me cook tea Indian style in Hindi!\")\n",
        "]\n",
        "\n",
        "response = chat.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gtSA6_Sg4fpP",
        "outputId": "d8aeb402-92c4-40f3-ab1f-330ab56d0dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arre bhai, chai banane ki recipe hai, lekin Hindi mein! (Hey brother, I've got a tea recipe for you, but in Hindi!)\n",
            "\n",
            "Chalo, shuru karte hain! (Let's get started!)\n",
            "\n",
            "Sabse pehle, aapko chahiye: (First, you'll need)\n",
            "\n",
            "* 1 cup pani (water)\n",
            "* 1/2 cup doodh (milk)\n",
            "* 1 chammach chai patti (1 teaspoon tea leaves)\n",
            "* 1 chammach cheeni (1 teaspoon sugar)\n",
            "* 1/4 chammach adrak (1/4 teaspoon ginger)\n",
            "* 1/4 chammach ilaichi (1/4 teaspoon cardamom)\n",
            "* 1/4 chammach dalchini (1/4 teaspoon cinnamon)\n",
            "\n",
            "Ab, chai banane ka tareeka hai: (Now, here's how to make the tea)\n",
            "\n",
            "1. Ek bartan mein pani ko ubaal lein. (Boil the water in a pot.)\n",
            "2. Usmein chai patti, adrak, ilaichi, aur dalchini daalein. (Add the tea leaves, ginger, cardamom, and cinnamon to the water.)\n",
            "3. 2-3 minute tak ubaalne dein. (Let it boil for 2-3 minutes.)\n",
            "4. Phir, doodh aur cheeni daalein. (Then, add the milk and sugar.)\n",
            "5. 1-2 minute tak aur ubaal lein. (Boil for another 1-2 minutes.)\n",
            "6. Chai ko chhan lein aur garmagaram parosein. (Strain the tea and serve it hot.)\n",
            "\n",
            "Arre, ab aapki chai taiyaar hai! (Now, your tea is ready!) Enjoy, bhai!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Email prompt Templates"
      ],
      "metadata": {
        "id": "pAxuAQUs9OoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "email_template = PromptTemplate.from_template(\n",
        "  \"Create an invitation email to the recipient that is {recipient_name}\\\n",
        "  for an event that is {event_type}\\\n",
        "  in a language that is {language}\\\n",
        "  Mention the event location that is {event_location}\\\n",
        "  and event date that is {event_date}.\\\n",
        "  Also write few sentences about the event description that is {event_description}\\\n",
        "  in style that is {style}.\\\n",
        "  Sender is {sender}\"\n",
        ")\n",
        "\n",
        "details = {\n",
        "  \"recipient_name\":\"Meetu\",\n",
        "  \"event_type\":\"Health Check\",\n",
        "  \"language\": \"Indian Hindi\",\n",
        "  \"event_location\":\"sarkari hospital, ghantaghar\",\n",
        "  \"event_date\":\"11 AM, January 15, 2026\",\n",
        "  \"event_description\":\"full body check up camp for free\",\n",
        "  \"style\":\"enthusiastic tone\",\n",
        "  \"sender\": \"CMO\"\n",
        "}\n",
        "\n",
        "\n",
        "prompt = email_template.invoke(details)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ra-ae5Wy5DXy",
        "outputId": "0f637154-5d99-4a08-a32b-03301ec6e045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Create an invitation email to the recipient that is Meetu  for an event that is Health Check  in a language that is Indian Hindi  Mention the event location that is sarkari hospital, ghantaghar  and event date that is 11 AM, January 15, 2026.  Also write few sentences about the event description that is full body check up camp for free  in style that is enthusiastic tone.  Sender is CMO'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JO2Qv3Tp7F0-",
        "outputId": "65747111-f1bf-4739-e71c-561f8ddd2b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "विषय: निःशुल्क स्वास्थ्य जांच शिविर - साक्षी बनें अपने स्वास्थ्य के संरक्षक!\n",
            "\n",
            "प्रिय मीतू,\n",
            "\n",
            "आपको हमारे निःशुल्क स्वास्थ्य जांच शिविर में आमंत्रित किया जाता है, जो घंटाघर स्थित सरकारी अस्पताल में आयोजित किया जा रहा है! यह शिविर आपके स्वास्थ्य की जांच करने का एक अद्भुत अवसर है, और हमें आपको इसमें शामिल होने के लिए आमंत्रित करने में खुशी हो रही है!\n",
            "\n",
            "तारीख: १५ जनवरी २०२६\n",
            "समय: ११ बजे प्रातः\n",
            "स्थान: सरकारी अस्पताल, घंटाघर\n",
            "\n",
            "हमारा यह स्वास्थ्य जांच शिविर पूर्ण शरीर जांच के लिए एक अनोखा अवसर प्रदान करता है, जिसमें आपको विशेषज्ञ डॉक्टरों द्वारा जांच की जाएगी और आपको अपने स्वास्थ्य संबंधी सभी समस्याओं का समाधान मिलेगा! यह शिविर पूर्णतः निःशुल्क है, इसलिए आपको इसके लिए कोई शुल्क नहीं देना होगा!\n",
            "\n",
            "हमें विश्वास है कि यह शिविर आपके लिए बहुत फायदेमंद होगा, और हम आपको इसमें शामिल होने के लिए आमंत्रित करते हैं! तो आइए, अपने स्वास्थ्य की जांच कराएं और स्वस्थ जीवन की ओर बढ़ें!\n",
            "\n",
            "आपका,\n",
            "सीएमओ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing of response"
      ],
      "metadata": {
        "id": "p3YlHF4t9TtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Date time Parser with Pydantic"
      ],
      "metadata": {
        "id": "oU2NZazk9XDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "\n",
        "class parsedOutput(BaseModel):\n",
        "  datetime: str = Field(description = \"ISO 8601 datetime string\")\n",
        "\n",
        "\n",
        "parser_date_time = PydanticOutputParser(pydantic_object =parsedOutput )\n",
        "# model_with_structure = model.with_structured_output(parsedOutput)\n",
        "prompt_datetime = PromptTemplate.from_template(\n",
        "    template = (\"Answer the question.\\n\"\n",
        "              \"{format_instructions}\\n\"\n",
        "              \"{question}\"\n",
        "              ),\n",
        "    # input_variables = [\"question\"] ,\n",
        "    partial_variables = {\"format_instructions\" : parser_date_time.get_format_instructions()}\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "prompt_value = prompt_datetime.invoke({\"question\": \"When was langchain released?\"})\n",
        "\n",
        "response = chat.invoke(prompt_value)\n",
        "\n",
        "print(response.content)\n",
        "returned_object = parser_date_time.parse(response.content)\n",
        "print(type(returned_object))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAcHtAL386tk",
        "outputId": "3ca8d3b7-2277-4adc-d455-093a4cc69c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"datetime\": \"2022-05-16T00:00:00Z\"}\n",
            "<class '__main__.parsedOutput'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV Parser"
      ],
      "metadata": {
        "id": "HQS3R9qP9a86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "class CSVParser(BaseModel):\n",
        "  csvoutput: List[str] = Field(description = \"Comma Separated values\")\n",
        "\n",
        "parser_csv = PydanticOutputParser(pydantic_object = CSVParser)\n",
        "\n",
        "prompt_csv = PromptTemplate.from_template(\n",
        "    template = (\"Answer the question.\\n\"\n",
        "              \"{format_instructions}\\n\"\n",
        "              \"{question}\"\n",
        "              ),\n",
        "    # input_variables = [\"question\"] ,\n",
        "    partial_variables = {\"format_instructions\" : parser_csv.get_format_instructions()}\n",
        "\n",
        ")\n",
        "prompt_value = prompt_csv.invoke({\"question\": \"What are the main important frameworks for building RAG system ?\"})\n",
        "\n",
        "response = chat.invoke(prompt_value)\n",
        "\n",
        "print(response.content)\n",
        "returned_object = parser_csv.parse(response.content)\n",
        "print(type(returned_object))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RAPZqhM9gX9",
        "outputId": "8d2667bb-504b-4197-a880-af9e305c7cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the list of main important frameworks for building a Retrieval-Augmented Generation (RAG) system, formatted as a JSON instance that conforms to the provided schema:\n",
            "\n",
            "```\n",
            "{\n",
            "  \"csvoutput\": [\n",
            "    \"Transformers\",\n",
            "    \"Hugging Face\",\n",
            "    \"PyTorch\",\n",
            "    \"TensorFlow\",\n",
            "    \"Stanford CoreNLP\",\n",
            "    \"Spacy\",\n",
            "    \"Gensim\",\n",
            "    \"Django\",\n",
            "    \"Flask\"\n",
            "  ]\n",
            "}\n",
            "```\n",
            "<class '__main__.CSVParser'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing thorugh .with_structured_output()"
      ],
      "metadata": {
        "id": "wbcTl6XZNm9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Author(BaseModel):\n",
        "    name: str = Field(description=\"The name of the author as a string.\")\n",
        "    number: int = Field(description=\"The number of books as an INTEGER, not a string.\")\n",
        "    books: list[str] = Field(description=\"A JSON ARRAY of book titles (strings). Do NOT return a stringified array.\")\n",
        "\n",
        "\n",
        "\n",
        "structured_llm = chat.with_structured_output(Author)\n",
        "\n",
        "returned_object = structured_llm.invoke(\n",
        "    \"Generate the books written by Dan Brown. \"\n",
        "    \"Return 'number' as an integer (not a string) and 'books' as a JSON array of strings (not a quoted string).\"\n",
        ")\n",
        "\n",
        "print(f\"{returned_object.name} wrote {returned_object.number} books.\")\n",
        "print(returned_object.books)\n",
        "\n",
        "\n",
        "#Throws error because Huggingface doesn't support function calling.\n",
        "#It can be done through openAI, Azure"
      ],
      "metadata": {
        "id": "JRW9ybGy_wXh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "collapsed": true,
        "outputId": "835fdff3-9d69-43ab-f75c-c1022605658e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Pydantic schema is not supported for function calling",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1455843068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstructured_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_structured_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAuthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m returned_object = structured_llm.invoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_huggingface/chat_models/huggingface.py\u001b[0m in \u001b[0;36mwith_structured_output\u001b[0;34m(self, schema, method, include_raw, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_pydantic_schema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Pydantic schema is not supported for function calling\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m             output_parser: JsonOutputKeyToolsParser | JsonOutputParser = (\n\u001b[1;32m   1176\u001b[0m                 \u001b[0mJsonOutputKeyToolsParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_tool_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Pydantic schema is not supported for function calling"
          ]
        }
      ]
    }
  ]
}